{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                 dataset_name='wiki_multilingual_cased',\n",
    "                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                 use_decoder=False, use_classifier=False)\n",
    "\n",
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 64\n",
    "\n",
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "            \n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "setattr(mod, 'dataset_train{}'.format(i), \n",
    "        nlp.data.TSVDataset(\n",
    "    \"./smishing_text/train_smishing{}.txt\".format(i), \n",
    "    field_indices=[1,2], num_discard_samples=1))\n",
    "setattr(mod, 'dataset_valid{}'.format(i),\n",
    "        nlp.data.TSVDataset(\n",
    "    \"./smishing_text/valid_smishing{}.txt\".format(i), \n",
    "    field_indices=[1,2], num_discard_samples=1))\n",
    "    \n",
    "\n",
    "setattr(mod, 'data_train{}'.format(i), \n",
    "        BERTDataset(\n",
    "            getattr(mod, 'dataset_train{}'.format(i)), \n",
    "            0, 1, bert_tokenizer, max_len, True, False))\n",
    "setattr(mod, 'data_valid{}'.format(i),\n",
    "       BERTDataset(\n",
    "           getattr(mod, 'dataset_valid{}'.format(i)), \n",
    "           0, 1, bert_tokenizer, max_len, True, False))\n",
    "    \n",
    "\n",
    "model_ = 'model{}'.format(i)\n",
    "setattr(mod, model_,\n",
    "        BERTClassifier(bert_base, num_classes=2, dropout=0.3))\n",
    "\n",
    "# 분류 레이어만 초기화 한다. \n",
    "getattr(mod, model_).classifier.initialize(ctx=ctx)\n",
    "getattr(mod, model_).hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "batch_size = 64\n",
    "lr = 5e-5\n",
    "\n",
    "\n",
    "setattr(mod, 'train_dataloader{}'.format(i),\n",
    "        mx.gluon.data.DataLoader(\n",
    "            getattr(mod, 'data_train{}'.format(i)), \n",
    "            batch_size=batch_size, num_workers=0))\n",
    "setattr(mod, 'valid_dataloader{}'.format(i),\n",
    "        mx.gluon.data.DataLoader(\n",
    "            getattr(mod, 'data_valid{}'.format(i)), \n",
    "            batch_size=batch_size, num_workers=0))\n",
    "    \n",
    "log_interval = 4\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "model_ = getattr(mod, 'model{}'.format(i))\n",
    "setattr(mod, 'trainer{}'.format(i), gluon.Trainer(\n",
    "    model_.collect_params(), \n",
    "    'bertadam',\n",
    "    {'learning_rate': lr, \n",
    "     'epsilon': 1e-9, \n",
    "     'wd':0.01}))\n",
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "iters = model_.collect_params('.*beta|.*gamma|.*bias').items()\n",
    "for _, v in iters:\n",
    "    v.wd_mult = 0.0\n",
    "        \n",
    "\n",
    "model_ = getattr(mod, 'model{}'.format(i))\n",
    "setattr(mod, 'params{}'.format(i),\n",
    "       [p for p in model_.collect_params().values() \n",
    "          if p.grad_req != 'null']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate warmup을 위한 준비 \n",
    "step_size = batch_size \n",
    "num_train_examples = len(getattr(mod, 'data_train{}'.format(i)))\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    ix = 0\n",
    "    for ix, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if ix > 1000:\n",
    "            break\n",
    "        ix += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2번째 모델\n",
      "[Epoch 1 Batch 50/482] loss=3.5441, lr=0.0000104167, acc=0.875\n",
      "[Epoch 1 Batch 100/482] loss=1.0443, lr=0.0000208333, acc=0.925\n",
      "[Epoch 1 Batch 150/482] loss=1.1543, lr=0.0000312500, acc=0.943\n",
      "[Epoch 1 Batch 200/482] loss=0.7825, lr=0.0000416667, acc=0.953\n",
      "[Epoch 1 Batch 250/482] loss=1.0256, lr=0.0000497692, acc=0.958\n",
      "[Epoch 1 Batch 300/482] loss=0.6390, lr=0.0000486150, acc=0.963\n",
      "[Epoch 1 Batch 350/482] loss=0.7012, lr=0.0000474608, acc=0.966\n",
      "[Epoch 1 Batch 400/482] loss=1.0592, lr=0.0000463066, acc=0.969\n",
      "[Epoch 1 Batch 450/482] loss=0.8977, lr=0.0000451524, acc=0.970\n",
      "Valid acc : 0.9880550506361984\n",
      "\n",
      "[Epoch 2 Batch 50/482] loss=0.7283, lr=0.0000432595, acc=0.989\n",
      "[Epoch 2 Batch 100/482] loss=0.3041, lr=0.0000421053, acc=0.993\n",
      "[Epoch 2 Batch 150/482] loss=0.2937, lr=0.0000409511, acc=0.994\n",
      "[Epoch 2 Batch 200/482] loss=0.3019, lr=0.0000397969, acc=0.994\n",
      "[Epoch 2 Batch 250/482] loss=0.3084, lr=0.0000386427, acc=0.994\n",
      "[Epoch 2 Batch 300/482] loss=0.3414, lr=0.0000374885, acc=0.994\n",
      "[Epoch 2 Batch 350/482] loss=0.2156, lr=0.0000363343, acc=0.995\n",
      "[Epoch 2 Batch 400/482] loss=0.2637, lr=0.0000351801, acc=0.995\n",
      "[Epoch 2 Batch 450/482] loss=0.1297, lr=0.0000340259, acc=0.995\n",
      "Valid acc : 0.9977927810958193\n",
      "\n",
      "[Epoch 3 Batch 50/482] loss=0.1937, lr=0.0000321330, acc=0.998\n",
      "[Epoch 3 Batch 100/482] loss=0.2067, lr=0.0000309788, acc=0.997\n",
      "[Epoch 3 Batch 150/482] loss=0.2210, lr=0.0000298246, acc=0.997\n",
      "[Epoch 3 Batch 200/482] loss=0.1601, lr=0.0000286704, acc=0.997\n",
      "[Epoch 3 Batch 250/482] loss=0.1683, lr=0.0000275162, acc=0.997\n",
      "[Epoch 3 Batch 300/482] loss=0.0536, lr=0.0000263620, acc=0.997\n",
      "[Epoch 3 Batch 350/482] loss=0.1713, lr=0.0000252078, acc=0.997\n",
      "[Epoch 3 Batch 400/482] loss=0.1471, lr=0.0000240536, acc=0.997\n",
      "[Epoch 3 Batch 450/482] loss=0.1057, lr=0.0000228994, acc=0.998\n",
      "Valid acc : 0.9987016359387172\n",
      "\n",
      "[Epoch 4 Batch 50/482] loss=0.1292, lr=0.0000210065, acc=0.998\n",
      "[Epoch 4 Batch 100/482] loss=0.0740, lr=0.0000198523, acc=0.999\n",
      "[Epoch 4 Batch 150/482] loss=0.1718, lr=0.0000186981, acc=0.998\n",
      "[Epoch 4 Batch 200/482] loss=0.1616, lr=0.0000175439, acc=0.998\n",
      "[Epoch 4 Batch 250/482] loss=0.0678, lr=0.0000163897, acc=0.999\n",
      "[Epoch 4 Batch 300/482] loss=0.0460, lr=0.0000152355, acc=0.999\n",
      "[Epoch 4 Batch 350/482] loss=0.1444, lr=0.0000140813, acc=0.999\n",
      "[Epoch 4 Batch 400/482] loss=0.0651, lr=0.0000129271, acc=0.999\n",
      "[Epoch 4 Batch 450/482] loss=0.0661, lr=0.0000117729, acc=0.999\n",
      "Valid acc : 0.9984419631264606\n",
      "\n",
      "[Epoch 5 Batch 50/482] loss=0.0589, lr=0.0000098800, acc=0.999\n",
      "[Epoch 5 Batch 100/482] loss=0.0574, lr=0.0000087258, acc=0.999\n",
      "[Epoch 5 Batch 150/482] loss=0.0975, lr=0.0000075716, acc=0.999\n",
      "[Epoch 5 Batch 200/482] loss=0.1672, lr=0.0000064174, acc=0.999\n",
      "[Epoch 5 Batch 250/482] loss=0.0471, lr=0.0000052632, acc=0.999\n",
      "[Epoch 5 Batch 300/482] loss=0.0332, lr=0.0000041090, acc=0.999\n",
      "[Epoch 5 Batch 350/482] loss=0.1358, lr=0.0000029548, acc=0.999\n",
      "[Epoch 5 Batch 400/482] loss=0.0193, lr=0.0000018006, acc=0.999\n",
      "[Epoch 5 Batch 450/482] loss=0.0220, lr=0.0000006464, acc=0.999\n",
      "Valid acc : 0.9990911451571021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{}번째 모델'.format(i))\n",
    "model = getattr(mod, 'model{}'.format(i))\n",
    "trainer = getattr(mod, 'trainer{}'.format(i))\n",
    "train_dataloader = getattr(mod, 'train_dataloader{}'.format(i))\n",
    "valid_dataloader = getattr(mod, 'valid_dataloader{}'.format(i))\n",
    "params = getattr(mod, 'params{}'.format(i))\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, \n",
    "                   valid_length, \n",
    "                   segment_ids, \n",
    "                   label) in enumerate(train_dataloader):\n",
    "        step_num += 1\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = (step_num - num_warmup_steps) * lr / (\n",
    "                num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        with mx.autograd.record():\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, \n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(token_ids.shape[0])\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                         .format(epoch_id + 1, batch_id + 1, \n",
    "                                 len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, \n",
    "                                 metric.get()[1]))\n",
    "            step_loss = 0\n",
    "    valid_acc = evaluate_accuracy(model, valid_dataloader, ctx)\n",
    "    print('Valid acc : {}\\n'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(mod, 'model{}'.format(i)).save_parameters('model{}.params'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_net = BERTClassifier(bert_base, num_classes=2, dropout=0.3)\n",
    "# new_net.load_parameters('model2.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
