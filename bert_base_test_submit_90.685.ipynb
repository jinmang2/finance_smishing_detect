{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                 dataset_name='wiki_multilingual_cased',\n",
    "                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                 use_decoder=False, use_classifier=False)\n",
    "\n",
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 64\n",
    "\n",
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "            \n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "setattr(mod, 'dataset_train{}'.format(i), \n",
    "        nlp.data.TSVDataset(\n",
    "    \"./smishing_text/train_smishing{}.txt\".format(i), \n",
    "    field_indices=[1,2], num_discard_samples=1))\n",
    "setattr(mod, 'dataset_valid{}'.format(i),\n",
    "        nlp.data.TSVDataset(\n",
    "    \"./smishing_text/valid_smishing{}.txt\".format(i), \n",
    "    field_indices=[1,2], num_discard_samples=1))\n",
    "    \n",
    "\n",
    "setattr(mod, 'data_train{}'.format(i), \n",
    "        BERTDataset(\n",
    "            getattr(mod, 'dataset_train{}'.format(i)), \n",
    "            0, 1, bert_tokenizer, max_len, True, False))\n",
    "setattr(mod, 'data_valid{}'.format(i),\n",
    "       BERTDataset(\n",
    "           getattr(mod, 'dataset_valid{}'.format(i)), \n",
    "           0, 1, bert_tokenizer, max_len, True, False))\n",
    "    \n",
    "\n",
    "model_ = 'model{}'.format(i)\n",
    "setattr(mod, model_,\n",
    "        BERTClassifier(bert_base, num_classes=2, dropout=0.3))\n",
    "\n",
    "# 분류 레이어만 초기화 한다. \n",
    "getattr(mod, model_).classifier.initialize(ctx=ctx)\n",
    "getattr(mod, model_).hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "batch_size = 64\n",
    "lr = 5e-5\n",
    "\n",
    "\n",
    "setattr(mod, 'train_dataloader{}'.format(i),\n",
    "        mx.gluon.data.DataLoader(\n",
    "            getattr(mod, 'data_train{}'.format(i)), \n",
    "            batch_size=batch_size, num_workers=0))\n",
    "setattr(mod, 'valid_dataloader{}'.format(i),\n",
    "        mx.gluon.data.DataLoader(\n",
    "            getattr(mod, 'data_valid{}'.format(i)), \n",
    "            batch_size=batch_size, num_workers=0))\n",
    "    \n",
    "log_interval = 4\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "model_ = getattr(mod, 'model{}'.format(i))\n",
    "setattr(mod, 'trainer{}'.format(i), gluon.Trainer(\n",
    "    model_.collect_params(), \n",
    "    'bertadam',\n",
    "    {'learning_rate': lr, \n",
    "     'epsilon': 1e-9, \n",
    "     'wd':0.01}))\n",
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "iters = model_.collect_params('.*beta|.*gamma|.*bias').items()\n",
    "for _, v in iters:\n",
    "    v.wd_mult = 0.0\n",
    "        \n",
    "\n",
    "model_ = getattr(mod, 'model{}'.format(i))\n",
    "setattr(mod, 'params{}'.format(i),\n",
    "       [p for p in model_.collect_params().values() \n",
    "          if p.grad_req != 'null']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate warmup을 위한 준비 \n",
    "step_size = batch_size \n",
    "num_train_examples = len(getattr(mod, 'data_train{}'.format(i)))\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    ix = 0\n",
    "    for ix, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if ix > 1000:\n",
    "            break\n",
    "        ix += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 모델\n",
      "[Epoch 1 Batch 50/482] loss=3.5184, lr=0.0000104167, acc=0.859\n",
      "[Epoch 1 Batch 100/482] loss=2.5072, lr=0.0000208333, acc=0.902\n",
      "[Epoch 1 Batch 150/482] loss=0.5082, lr=0.0000312500, acc=0.931\n",
      "[Epoch 1 Batch 200/482] loss=1.4466, lr=0.0000416667, acc=0.940\n",
      "[Epoch 1 Batch 250/482] loss=1.4656, lr=0.0000497692, acc=0.947\n",
      "[Epoch 1 Batch 300/482] loss=0.6288, lr=0.0000486150, acc=0.954\n",
      "[Epoch 1 Batch 350/482] loss=0.8557, lr=0.0000474608, acc=0.959\n",
      "[Epoch 1 Batch 400/482] loss=1.1062, lr=0.0000463066, acc=0.961\n",
      "[Epoch 1 Batch 450/482] loss=0.4469, lr=0.0000451524, acc=0.965\n",
      "Valid acc : 0.9957153985977668\n",
      "\n",
      "[Epoch 2 Batch 50/482] loss=0.2195, lr=0.0000432595, acc=0.997\n",
      "[Epoch 2 Batch 100/482] loss=0.2120, lr=0.0000421053, acc=0.997\n",
      "[Epoch 2 Batch 150/482] loss=0.2077, lr=0.0000409511, acc=0.997\n",
      "[Epoch 2 Batch 200/482] loss=0.2402, lr=0.0000397969, acc=0.996\n",
      "[Epoch 2 Batch 250/482] loss=0.1404, lr=0.0000386427, acc=0.997\n",
      "[Epoch 2 Batch 300/482] loss=0.1381, lr=0.0000374885, acc=0.997\n",
      "[Epoch 2 Batch 350/482] loss=0.1317, lr=0.0000363343, acc=0.997\n",
      "[Epoch 2 Batch 400/482] loss=0.3635, lr=0.0000351801, acc=0.997\n",
      "[Epoch 2 Batch 450/482] loss=0.0960, lr=0.0000340259, acc=0.997\n",
      "Valid acc : 0.997662944689691\n",
      "\n",
      "[Epoch 3 Batch 50/482] loss=0.1079, lr=0.0000321330, acc=0.998\n",
      "[Epoch 3 Batch 100/482] loss=0.0489, lr=0.0000309788, acc=0.998\n",
      "[Epoch 3 Batch 150/482] loss=0.1716, lr=0.0000298246, acc=0.998\n",
      "[Epoch 3 Batch 200/482] loss=0.1056, lr=0.0000286704, acc=0.998\n",
      "[Epoch 3 Batch 250/482] loss=0.0674, lr=0.0000275162, acc=0.998\n",
      "[Epoch 3 Batch 300/482] loss=0.0100, lr=0.0000263620, acc=0.998\n",
      "[Epoch 3 Batch 350/482] loss=0.1062, lr=0.0000252078, acc=0.998\n",
      "[Epoch 3 Batch 400/482] loss=0.1051, lr=0.0000240536, acc=0.998\n",
      "[Epoch 3 Batch 450/482] loss=0.0408, lr=0.0000228994, acc=0.999\n",
      "Valid acc : 0.9903921059465074\n",
      "\n",
      "[Epoch 4 Batch 50/482] loss=0.1386, lr=0.0000210065, acc=0.997\n",
      "[Epoch 4 Batch 100/482] loss=0.0012, lr=0.0000198523, acc=0.999\n",
      "[Epoch 4 Batch 150/482] loss=0.0260, lr=0.0000186981, acc=0.999\n",
      "[Epoch 4 Batch 200/482] loss=0.0417, lr=0.0000175439, acc=0.999\n",
      "[Epoch 4 Batch 250/482] loss=0.0009, lr=0.0000163897, acc=0.999\n",
      "[Epoch 4 Batch 300/482] loss=0.0007, lr=0.0000152355, acc=0.999\n",
      "[Epoch 4 Batch 350/482] loss=0.0030, lr=0.0000140813, acc=0.999\n",
      "[Epoch 4 Batch 400/482] loss=0.1134, lr=0.0000129271, acc=0.999\n",
      "[Epoch 4 Batch 450/482] loss=0.0145, lr=0.0000117729, acc=0.999\n",
      "Valid acc : 0.9971435990651779\n",
      "\n",
      "[Epoch 5 Batch 50/482] loss=0.0171, lr=0.0000098800, acc=1.000\n",
      "[Epoch 5 Batch 100/482] loss=0.0007, lr=0.0000087258, acc=1.000\n",
      "[Epoch 5 Batch 150/482] loss=0.0007, lr=0.0000075716, acc=1.000\n",
      "[Epoch 5 Batch 200/482] loss=0.0032, lr=0.0000064174, acc=1.000\n",
      "[Epoch 5 Batch 250/482] loss=0.0005, lr=0.0000052632, acc=1.000\n",
      "[Epoch 5 Batch 300/482] loss=0.0006, lr=0.0000041090, acc=1.000\n",
      "[Epoch 5 Batch 350/482] loss=0.0006, lr=0.0000029548, acc=1.000\n",
      "[Epoch 5 Batch 400/482] loss=0.0168, lr=0.0000018006, acc=1.000\n",
      "[Epoch 5 Batch 450/482] loss=0.0005, lr=0.0000006464, acc=1.000\n",
      "Valid acc : 0.9980524539080758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{}번째 모델'.format(i))\n",
    "model = getattr(mod, 'model{}'.format(i))\n",
    "trainer = getattr(mod, 'trainer{}'.format(i))\n",
    "train_dataloader = getattr(mod, 'train_dataloader{}'.format(i))\n",
    "valid_dataloader = getattr(mod, 'valid_dataloader{}'.format(i))\n",
    "params = getattr(mod, 'params{}'.format(i))\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, \n",
    "                   valid_length, \n",
    "                   segment_ids, \n",
    "                   label) in enumerate(train_dataloader):\n",
    "        step_num += 1\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = (step_num - num_warmup_steps) * lr / (\n",
    "                num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        with mx.autograd.record():\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, \n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(token_ids.shape[0])\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                         .format(epoch_id + 1, batch_id + 1, \n",
    "                                 len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, \n",
    "                                 metric.get()[1]))\n",
    "            step_loss = 0\n",
    "    valid_acc = evaluate_accuracy(model, valid_dataloader, ctx)\n",
    "    print('Valid acc : {}\\n'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = nlp.data.TSVDataset(\n",
    "    \"./smishing_text/train_smishing1.txt\".format(i), \n",
    "    field_indices=[1,2], num_discard_samples=1)\n",
    "\n",
    "data_valid = BERTDataset(dataset_valid,\n",
    "        0, 1, bert_tokenizer, max_len, True, False)\n",
    "\n",
    "valid_dataloader2 = mx.gluon.data.DataLoader(\n",
    "        data_valid, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480"
     ]
    }
   ],
   "source": [
    "# model = nets[1]\n",
    "acc = mx.metric.Accuracy()\n",
    "result = []\n",
    "for ix, (t, v, s, label) in enumerate(valid_dataloader2):\n",
    "    if ix % 10 == 0:\n",
    "        print('\\r{}'.format(ix), end='')\n",
    "    token_ids = t.as_in_context(ctx)\n",
    "    valid_length = v.as_in_context(ctx)\n",
    "    segment_ids = s.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    output = model(token_ids, segment_ids, \n",
    "                   valid_length.astype('float32'))\n",
    "    result.append([output, label])\n",
    "    acc.update(preds=output, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('accuracy', 0.9999350733670952)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
       " 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.\n",
       " 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
       "<NDArray 64 @gpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.ndarray.argmax(result[0][0].softmax(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1\n",
       " 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0]\n",
       "<NDArray 64 @gpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('model_test.params'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for i in range(1, 15):\n",
    "    new_net = BERTClassifier(bert_base, num_classes=2, dropout=.3)\n",
    "    new_net.load_parameters(\n",
    "        'model{}.params'.format(i), ctx=ctx)\n",
    "    acc = mx.metric.Accuracy()\n",
    "    res = []\n",
    "    for ix, (t, v, s, label) in enumerate(valid_dataloader2):\n",
    "        if ix % 10 == 0:\n",
    "            print('\\r{}'.format(ix), end='')\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = new_net(token_ids, segment_ids, \n",
    "                       valid_length.astype('float32'))\n",
    "        res.append([output, label])\n",
    "        acc.update(preds=output, labels=label)\n",
    "    result[i] = (res, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('accuracy', 0.9991884170886898)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[14][1].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(482):\n",
    "    res.append(\n",
    "        mx.ndarray.argmax(result[2][0][i][0].softmax(), axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test셋 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dacon문자스미싱/filedown (2)'\n",
    "\n",
    "test = pd.read_csv(path + '/public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 937 ms\n"
     ]
    }
   ],
   "source": [
    "test = test.set_index('id')\n",
    "%time text = test.text.map(lambda x : mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.4 ms\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['XXX', '.', '을', '를', '이', '가', \n",
    "             '-', '(', ')', ':', '!', '?', ')-', \n",
    "             '.-', 'ㅡ', 'XXXXXX', '..', '.(', '은', '는'] #필요없는 단어 리스트\n",
    "%time text = text.map(lambda x : [_word for _word in x if _word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%time text = text.map(lambda x : ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "from chatspace import ChatSpace\n",
    "\n",
    "spacer = ChatSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%time text = text.map(lambda x : spacer.space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('(test)spacing_text_except_stopwords.pkl', 'wb') as f:\n",
    "    pickle.dump(text, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = text.map(lambda x : re.sub('X', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(text).to_csv(\n",
    "    './smishing_text/test_smishing.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset_test(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(\n",
    "    \"./smishing_text/test_smishing.txt\".format(i), \n",
    "    field_indices=[1], num_discard_samples=1)\n",
    "\n",
    "data_test = BERTDataset_test(dataset_test,\n",
    "        0, bert_tokenizer, max_len, True, False)\n",
    "\n",
    "test_dataloader = mx.gluon.data.DataLoader(\n",
    "        data_test, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\t20"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for i in range(1, 15):\n",
    "    new_net = BERTClassifier(bert_base, num_classes=2, dropout=.3)\n",
    "    if i == 1:\n",
    "            new_net.load_parameters(\n",
    "            'model{}.params'.format('_test'), ctx=ctx)\n",
    "    else:\n",
    "            new_net.load_parameters(\n",
    "            'model{}.params'.format(i), ctx=ctx)\n",
    "    res = []\n",
    "    for ix, (t, v, s) in enumerate(test_dataloader):\n",
    "        if ix % 10 == 0:\n",
    "            print('\\r{}\\t{}'.format(i, ix), end='')\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        output = new_net(token_ids, segment_ids, \n",
    "                       valid_length.astype('float32'))\n",
    "        res.append(output)\n",
    "    result[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_ix in range(1, 15):\n",
    "    res = []\n",
    "    for i in range(26):\n",
    "        res.append(result[model_ix][i].softmax().asnumpy())\n",
    "    res = np.concatenate(res)\n",
    "    results[model_ix] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "preds = np.concatenate(\n",
    "    [results[model_ix].reshape(-1, 2, 1) \n",
    "     for model_ix in range(1, 15)], axis=2).mean(axis=2)\n",
    "# Softmax\n",
    "preds = preds / preds.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../dacon문자스미싱/submission_제출양식.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['smishing'] = preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1478\n",
       "0     148\n",
       "Name: smishing, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.smishing.map(\n",
    "    lambda x : np.random.choice(2, p=(x, 1-x))).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340000</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340001</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>340002</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340003</td>\n",
       "      <td>0.000477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340004</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>341621</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>341622</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>341623</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>341624</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>341625</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  smishing\n",
       "0     340000  0.000504\n",
       "1     340001  0.000505\n",
       "2     340002  0.000489\n",
       "3     340003  0.000477\n",
       "4     340004  0.000479\n",
       "...      ...       ...\n",
       "1621  341621  0.000483\n",
       "1622  341622  0.000475\n",
       "1623  341623  0.000475\n",
       "1624  341624  0.000485\n",
       "1625  341625  0.000493\n",
       "\n",
       "[1626 rows x 2 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.set_index('id').to_csv('submission_bert_ensemble.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
