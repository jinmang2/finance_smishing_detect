{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                 dataset_name='wiki_multilingual_cased',\n",
    "                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                 use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2  8982  9356 47869  9566     3  8935 22333 38851     3]\n",
      "10\n",
      "[0 0 0 0 0 0 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = gluon.data.SimpleDataset([['나 보기가 역겨워', '김소월']])\n",
    "tok = nlp.data.BERTTokenizer(vocab=vocabulary, lower=False)\n",
    "trans = nlp.data.BERTSentenceTransform(tok, max_seq_length=10)\n",
    "\n",
    "[print(i) for i in list(ds.transform(trans))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"./nsmc/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"./nsmc/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nlp.data.TSVDataset('./train_smishing.txt', field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, bert_tokenizer, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, bert_tokenizer, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "            \n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bert_base, num_classes=2, dropout=0.3)\n",
    "# 분류 레이어만 초기화 한다. \n",
    "model.classifier.initialize(ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': lr, \n",
    "                         'epsilon': 1e-9, \n",
    "                         'wd':0.01})\n",
    "\n",
    "log_interval = 4\n",
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "    v.wd_mult = 0.0\n",
    "params = [\n",
    "    p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    i = 0\n",
    "    for i, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        i += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate warmup을 위한 준비 \n",
    "step_size = batch_size \n",
    "num_train_examples = len(data_train)\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Acc : 0.52686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a9ecc768a7cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTest Acc : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-1464d6ddb664>\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(model, data_iter, ctx)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msegment_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\gluon\\block.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a3e73949446c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, token_types, valid_length)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\gluonnlp\\model\\bert.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, token_types, valid_length, masked_positions)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mmasked_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmasked_positions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmasked_positions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         return super(BERTModel, self).__call__(inputs, token_types,\n\u001b[1;32m--> 429\u001b[1;33m                                                valid_length, masked_positions)\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhybrid_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked_positions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\gluon\\block.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\gluon\\block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_cached_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\gluon\\block.py\u001b[0m in \u001b[0;36m_call_cached_op\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    819\u001b[0m                     \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish_deferred_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                     \u001b[0mcargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             ctypes.byref(out_stypes)))\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, \n",
    "                   valid_length, \n",
    "                   segment_ids, \n",
    "                   label) in enumerate(train_dataloader):\n",
    "        step_num += 1\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = (step_num - num_warmup_steps) * lr / (\n",
    "                num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        with mx.autograd.record():\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, \n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(token_ids.shape[0])\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print('\\r[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                         .format(epoch_id + 1, batch_id + 1, \n",
    "                                 len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, \n",
    "                                 metric.get()[1]),\n",
    "                 end='')\n",
    "            step_loss = 0\n",
    "        elif batch_id == 10:\n",
    "            break\n",
    "    test_acc = evaluate_accuracy(model, test_dataloader, ctx)\n",
    "    print('\\nTest Acc : {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.40061206  0.35546818]\n",
       " [ 0.32571524  0.2718605 ]\n",
       " [ 0.13055156  0.6908209 ]\n",
       " [ 0.11203045  0.41319743]\n",
       " [ 0.135449    0.34487763]\n",
       " [ 0.25897866  1.0362484 ]\n",
       " [ 0.45636153  0.18079619]\n",
       " [ 0.2402434   0.4008932 ]\n",
       " [ 0.12857051  0.08971198]\n",
       " [-0.21496539  0.14349057]\n",
       " [ 0.3352273   0.22073273]\n",
       " [ 0.05570364  0.7990796 ]\n",
       " [ 0.25062743 -0.07154827]\n",
       " [ 0.2273978   0.36972913]\n",
       " [ 0.15171419  0.14698373]\n",
       " [-0.02592476 -0.12936436]\n",
       " [-0.11573608  0.5158846 ]\n",
       " [ 0.21563885  0.4094187 ]\n",
       " [ 0.31336477 -0.14875774]\n",
       " [ 0.17630681  0.25964037]\n",
       " [ 0.3710956   0.30664656]\n",
       " [ 0.50876075  0.5201391 ]\n",
       " [ 0.06279844  0.4675638 ]\n",
       " [ 0.24566874  0.70804137]\n",
       " [-0.08556962  0.4800857 ]\n",
       " [ 0.3667487   0.6270006 ]\n",
       " [ 0.37454385 -0.11083919]\n",
       " [ 0.35946134  0.64396936]\n",
       " [-0.13175201  0.24146204]\n",
       " [ 0.27762577  0.26266006]\n",
       " [ 0.35406485  0.19143829]\n",
       " [ 0.29076728  0.00792346]\n",
       " [ 0.49147323  0.32852086]\n",
       " [-0.05769968 -0.14159451]\n",
       " [ 0.46907485  0.559992  ]\n",
       " [ 0.13862616 -0.05649821]\n",
       " [ 0.3221844   0.5495527 ]\n",
       " [ 0.10503846  0.4678007 ]\n",
       " [ 0.20964254  0.7195428 ]\n",
       " [ 0.2580801   0.7150082 ]\n",
       " [ 0.17820938  0.7947888 ]\n",
       " [ 0.29063752  0.22510748]\n",
       " [ 0.19380923  0.1714469 ]\n",
       " [ 0.3216776   0.2524634 ]\n",
       " [ 0.28243104  0.15989056]\n",
       " [ 0.24533318  0.3381415 ]\n",
       " [-0.08276047  0.5002615 ]\n",
       " [-0.00946501  0.38423863]\n",
       " [ 0.6154185   0.4632067 ]\n",
       " [ 0.49041623  0.4083569 ]\n",
       " [ 0.20363182  0.7452281 ]\n",
       " [ 0.27022037  0.2778802 ]\n",
       " [-0.01760279  0.24204151]\n",
       " [ 0.47013843  0.471799  ]\n",
       " [-0.05027728  0.37063792]\n",
       " [ 0.25855234  0.33390492]\n",
       " [ 0.41693345  0.19153252]\n",
       " [-0.02425477  0.29529193]\n",
       " [-0.00610711  0.14274082]\n",
       " [ 0.2838679   0.15441094]\n",
       " [ 0.3403608   0.14726815]\n",
       " [-0.16765535  0.6953154 ]\n",
       " [ 0.33576944  0.12647322]\n",
       " [ 0.15932426  0.44992045]]\n",
       "<NDArray 64x2 @gpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.dataloader.DataLoader at 0x20b77531860>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Smishing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dacon문자스미싱/filedown (2)'\n",
    "\n",
    "train = pd.read_csv(path + '/train.csv')\n",
    "test = pd.read_csv(path + '/public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year_month</th>\n",
       "      <th>text</th>\n",
       "      <th>smishing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX은행성산XXX팀장입니다.행복한주말되세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295940</th>\n",
       "      <td>336373</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>XXX 고객님!열심히 달려왔던 2018년도 어느 새 뒤안길로 지나쳐가고 벅찬 설렘으...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295941</th>\n",
       "      <td>336375</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>XXX고객님실버웰빙신탁이 만기도래 예정입니다.XXX남양주지점</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295942</th>\n",
       "      <td>336376</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>한해동안 XXX은행과 함께 해주셔서 정말 감사드립니다 2019년 기해년을 맞이하며 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295943</th>\n",
       "      <td>336377</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>1228(금)예금및 적금 만기입니다.예금은 시간내서 내점하시고 적금은 1년 자동연장...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295944</th>\n",
       "      <td>336378</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>안녕하세요 XXX 고객님. 스타링크 전담직원 XXX입니다. 스타링크 고객님 대상으로...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295945 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id year_month                                               text  \\\n",
       "0            0    2017-01                           XXX은행성산XXX팀장입니다.행복한주말되세요   \n",
       "1            1    2017-01              오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림   \n",
       "2            2    2017-01  안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...   \n",
       "3            4    2017-01  XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...   \n",
       "4            5    2017-01           1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다   \n",
       "...        ...        ...                                                ...   \n",
       "295940  336373    2018-12  XXX 고객님!열심히 달려왔던 2018년도 어느 새 뒤안길로 지나쳐가고 벅찬 설렘으...   \n",
       "295941  336375    2018-12                  XXX고객님실버웰빙신탁이 만기도래 예정입니다.XXX남양주지점   \n",
       "295942  336376    2018-12  한해동안 XXX은행과 함께 해주셔서 정말 감사드립니다 2019년 기해년을 맞이하며 ...   \n",
       "295943  336377    2018-12  1228(금)예금및 적금 만기입니다.예금은 시간내서 내점하시고 적금은 1년 자동연장...   \n",
       "295944  336378    2018-12  안녕하세요 XXX 고객님. 스타링크 전담직원 XXX입니다. 스타링크 고객님 대상으로...   \n",
       "\n",
       "        smishing  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "295940         0  \n",
       "295941         0  \n",
       "295942         0  \n",
       "295943         0  \n",
       "295944         0  \n",
       "\n",
       "[295945 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = train.set_index('id')\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                 train['text'], train['smishing'],\n",
    "                 random_state=42, test_size=.2,\n",
    "                 stratify=train['smishing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((236756,), (59189,), (236756,), (59189,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    221794\n",
       "1     14962\n",
       "Name: smishing, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    55448\n",
       "1     3741\n",
       "Name: smishing, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((X_train, y_train), axis=1).to_csv(\n",
    "    'train_smishing.txt', sep='\\t')\n",
    "pd.concat((X_valid, y_valid), axis=1).to_csv(\n",
    "    'valid_smishing.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                 dataset_name='wiki_multilingual_cased',\n",
    "                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                 use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"./train_smishing.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_valid = nlp.data.TSVDataset(\"./valid_smishing.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, bert_tokenizer, max_len, True, False)\n",
    "data_valid = BERTDataset(dataset_valid, 0, 1, bert_tokenizer, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "            \n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "@mx.metric.register\n",
    "@mx.metric.alias('auc')\n",
    "class AUCMetric(mx.metric.EvalMetric):\n",
    "    def __init__(self, eps=1e-12):\n",
    "        super(AUCMetric, self).__init__(\n",
    "            'auc')\n",
    "        self.eps = eps\n",
    "\n",
    "    def update(self, labels, preds):\n",
    "        mx.metric.check_label_shapes(labels, preds)\n",
    "        label_weight = labels[0].asnumpy()\n",
    "        preds = preds[0].asnumpy()\n",
    "        tmp = []\n",
    "        for i in range(preds.shape[0]):\n",
    "            tmp.append((label_weight[i], preds[i]))\n",
    "#         tmp = sorted(tmp, key=itemgetter(1), reverse=True)\n",
    "        label_sum = label_weight.sum()\n",
    "        if label_sum == 0 or label_sum == label_weight.size:\n",
    "            raise Exception(\"AUC with one class is undefined\")\n",
    "\n",
    "        label_one_num = np.count_nonzero(label_weight)\n",
    "        label_zero_num = len(label_weight) - label_one_num\n",
    "        total_area = label_zero_num * label_one_num\n",
    "        height = 0\n",
    "        width = 0\n",
    "        area = 0\n",
    "        for a, _ in tmp:\n",
    "            if a == 1.0:\n",
    "                height += 1.0\n",
    "            else:\n",
    "                width += 1.0\n",
    "                area += height\n",
    "\n",
    "        self.sum_metric += area / total_area\n",
    "        self.num_inst += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bert_base, num_classes=2, dropout=0.3)\n",
    "# 분류 레이어만 초기화 한다. \n",
    "model.classifier.initialize(ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "# metric = AUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "valid_dataloader = mx.gluon.data.DataLoader(data_valid, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': lr, \n",
    "                         'epsilon': 1e-9, \n",
    "                         'wd':0.01})\n",
    "\n",
    "log_interval = 4\n",
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "    v.wd_mult = 0.0\n",
    "params = [\n",
    "    p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate warmup을 위한 준비 \n",
    "step_size = batch_size \n",
    "num_train_examples = len(data_train)\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    i = 0\n",
    "    for i, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        i += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 50/3700] loss=4.1034, lr=0.0000016903, acc=0.873\n",
      "[Epoch 1 Batch 100/3700] loss=1.0404, lr=0.0000033807, acc=0.919\n",
      "[Epoch 1 Batch 150/3700] loss=0.6426, lr=0.0000050710, acc=0.941\n",
      "[Epoch 1 Batch 200/3700] loss=0.4843, lr=0.0000067613, acc=0.953\n",
      "[Epoch 1 Batch 250/3700] loss=0.4260, lr=0.0000084517, acc=0.960\n",
      "[Epoch 1 Batch 300/3700] loss=0.2746, lr=0.0000101420, acc=0.966\n",
      "[Epoch 1 Batch 350/3700] loss=0.4069, lr=0.0000118323, acc=0.970\n",
      "[Epoch 1 Batch 400/3700] loss=0.2231, lr=0.0000135227, acc=0.973\n",
      "[Epoch 1 Batch 450/3700] loss=0.1175, lr=0.0000152130, acc=0.976\n",
      "[Epoch 1 Batch 500/3700] loss=0.2951, lr=0.0000169033, acc=0.978\n",
      "[Epoch 1 Batch 550/3700] loss=0.2699, lr=0.0000185936, acc=0.979\n",
      "[Epoch 1 Batch 600/3700] loss=0.2785, lr=0.0000202840, acc=0.981\n",
      "[Epoch 1 Batch 650/3700] loss=0.2811, lr=0.0000219743, acc=0.982\n",
      "[Epoch 1 Batch 700/3700] loss=0.1271, lr=0.0000236646, acc=0.983\n",
      "[Epoch 1 Batch 750/3700] loss=0.1214, lr=0.0000253550, acc=0.984\n",
      "[Epoch 1 Batch 800/3700] loss=0.3340, lr=0.0000270453, acc=0.985\n",
      "[Epoch 1 Batch 850/3700] loss=0.2863, lr=0.0000287356, acc=0.985\n",
      "[Epoch 1 Batch 900/3700] loss=0.1167, lr=0.0000304260, acc=0.986\n",
      "[Epoch 1 Batch 950/3700] loss=0.3622, lr=0.0000321163, acc=0.986\n",
      "[Epoch 1 Batch 1000/3700] loss=0.3314, lr=0.0000338066, acc=0.987\n",
      "[Epoch 1 Batch 1050/3700] loss=0.3069, lr=0.0000354970, acc=0.987\n",
      "[Epoch 1 Batch 1100/3700] loss=0.2600, lr=0.0000371873, acc=0.987\n",
      "[Epoch 1 Batch 1150/3700] loss=0.1415, lr=0.0000388776, acc=0.988\n",
      "[Epoch 1 Batch 1200/3700] loss=0.1426, lr=0.0000405680, acc=0.988\n",
      "[Epoch 1 Batch 1250/3700] loss=0.2692, lr=0.0000422583, acc=0.988\n",
      "[Epoch 1 Batch 1300/3700] loss=0.2384, lr=0.0000439486, acc=0.989\n",
      "[Epoch 1 Batch 1350/3700] loss=0.1659, lr=0.0000456389, acc=0.989\n",
      "[Epoch 1 Batch 1400/3700] loss=0.3799, lr=0.0000473293, acc=0.989\n",
      "[Epoch 1 Batch 1450/3700] loss=0.1345, lr=0.0000490196, acc=0.990\n",
      "[Epoch 1 Batch 1500/3700] loss=0.1836, lr=0.0000499212, acc=0.990\n",
      "[Epoch 1 Batch 1550/3700] loss=3.0966, lr=0.0000497334, acc=0.988\n",
      "[Epoch 1 Batch 1600/3700] loss=3.0942, lr=0.0000495457, acc=0.987\n",
      "[Epoch 1 Batch 1650/3700] loss=3.4749, lr=0.0000493580, acc=0.985\n",
      "[Epoch 1 Batch 1700/3700] loss=3.0472, lr=0.0000491703, acc=0.983\n",
      "[Epoch 1 Batch 1750/3700] loss=3.0361, lr=0.0000489826, acc=0.982\n",
      "[Epoch 1 Batch 1800/3700] loss=3.1843, lr=0.0000487949, acc=0.981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-59aaf951198d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mstep_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\metric.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, labels, preds)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mpred_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0mpred_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# flatten before checking shapes to avoid shape miss match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, \n",
    "                   valid_length, \n",
    "                   segment_ids, \n",
    "                   label) in enumerate(train_dataloader):\n",
    "        step_num += 1\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = (step_num - num_warmup_steps) * lr / (\n",
    "                num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        with mx.autograd.record():\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, \n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(token_ids.shape[0])\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                         .format(epoch_id + 1, batch_id + 1, \n",
    "                                 len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, \n",
    "                                 metric.get()[1]))\n",
    "            step_loss = 0\n",
    "    valid_acc = evaluate_accuracy(model, valid_dataloader, ctx)\n",
    "    print('\\nValid acc : {}'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리해서 다시 도전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dacon문자스미싱/filedown (2)'\n",
    "\n",
    "train = pd.read_csv(path + '/train.csv')\n",
    "test = pd.read_csv(path + '/public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0                                  XXX은행성산XXX팀장입니다.행복한주말되세요\n",
       "1                     오늘도많이웃으시는하루시작하세요XXX은행 진월동VIP라운지 XXX올림\n",
       "2         안녕하십니까 고객님. XXX은행입니다.금일 납부하셔야 할 금액은 153600원 입니...\n",
       "4         XXX 고객님안녕하세요XXX은행 XXX지점입니다지난 한 해 동안 저희 XXX지점에 ...\n",
       "5                  1월은 새로움이 가득XXX입니다.올 한해 더 많이행복한 한해되시길바랍니다\n",
       "                                ...                        \n",
       "336373    XXX 고객님!열심히 달려왔던 2018년도 어느 새 뒤안길로 지나쳐가고 벅찬 설렘으...\n",
       "336375                    XXX고객님실버웰빙신탁이 만기도래 예정입니다.XXX남양주지점\n",
       "336376    한해동안 XXX은행과 함께 해주셔서 정말 감사드립니다 2019년 기해년을 맞이하며 ...\n",
       "336377    1228(금)예금및 적금 만기입니다.예금은 시간내서 내점하시고 적금은 1년 자동연장...\n",
       "336378    안녕하세요 XXX 고객님. 스타링크 전담직원 XXX입니다. 스타링크 고객님 대상으로...\n",
       "Name: text, Length: 295945, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%time text = train.text.map(lambda x : mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.12 s\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['XXX', '.', '을', '를', '이', '가', \n",
    "             '-', '(', ')', ':', '!', '?', ')-', \n",
    "             '.-', 'ㅡ', 'XXXXXX', '..', '.(', '은', '는'] #필요없는 단어 리스트\n",
    "%time text = text.map(lambda x : [_word for _word in x if _word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 871 ms\n"
     ]
    }
   ],
   "source": [
    "%time text = text.map(lambda x : ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "from chatspace import ChatSpace\n",
    "\n",
    "spacer = ChatSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 33min 13s\n"
     ]
    }
   ],
   "source": [
    "%time text = text.map(lambda x : spacer.space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('spacing_text_except_stopwords.pkl', 'wb') as f:\n",
    "    pickle.dump(text, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.concat((text, train['smishing']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = train2['smishing'].value_counts(normalize=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a // b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train2['text'] = train2.text.map(lambda x : re.sub('X', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = train2[train2['smishing'] == 0]\n",
    "abnormal = train2[train2['smishing'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(len(normal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(14):\n",
    "    train_data.append(\n",
    "        normal.iloc[ind[i*len(normal)//14:(i+1)*len(normal)//14]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    train_data[i] = \\\n",
    "        pd.concat((train_data[i], abnormal)).sort_index()\n",
    "    random_index = \\\n",
    "        np.random.permutation(len(normal) // 14 + len(abnormal))\n",
    "    train_data[i] = train_data[i].iloc[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in range(14):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                     train_data[i]['text'], \n",
    "                     train_data[i]['smishing'],\n",
    "                     test_size=.2,\n",
    "                     stratify=train_data[i]['smishing'])\n",
    "    setattr(mod, 'X_train{}'.format(i+1), X_train)\n",
    "    setattr(mod, 'X_valid{}'.format(i+1), X_valid)\n",
    "    setattr(mod, 'y_train{}'.format(i+1), y_train)\n",
    "    setattr(mod, 'y_valid{}'.format(i+1), y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    pd.concat((getattr(mod, 'X_train{}'.format(i)),\n",
    "               getattr(mod, 'y_train{}'.format(i))), \n",
    "              axis=1).to_csv(\n",
    "        './smishing_text/train_smishing{}.txt'.format(i), \n",
    "                    sep='\\t')\n",
    "    pd.concat((getattr(mod, 'X_valid{}'.format(i)),\n",
    "               getattr(mod, 'y_valid{}'.format(i))), \n",
    "              axis=1).to_csv(\n",
    "        './smishing_text/valid_smishing{}.txt'.format(i), \n",
    "                    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                 dataset_name='wiki_multilingual_cased',\n",
    "                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                 use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    setattr(mod, 'dataset_train{}'.format(i), \n",
    "            nlp.data.TSVDataset(\n",
    "        \"./smishing_text/train_smishing{}.txt\".format(i), \n",
    "        field_indices=[1,2], num_discard_samples=1))\n",
    "    setattr(mod, 'dataset_valid{}'.format(i),\n",
    "            nlp.data.TSVDataset(\n",
    "        \"./smishing_text/valid_smishing{}.txt\".format(i), \n",
    "        field_indices=[1,2], num_discard_samples=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, \n",
    "                 bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, \n",
    "            pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset(\n",
    "            [[i[sent_idx],] for i in dataset]\n",
    "        )\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    setattr(mod, 'data_train{}'.format(i), \n",
    "            BERTDataset(\n",
    "                getattr(mod, 'dataset_train{}'.format(i)), \n",
    "                0, 1, bert_tokenizer, max_len, True, False))\n",
    "    setattr(mod, 'data_valid{}'.format(i),\n",
    "           BERTDataset(\n",
    "               getattr(mod, 'dataset_valid{}'.format(i)), \n",
    "               0, 1, bert_tokenizer, max_len, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "            \n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    model_ = 'model{}'.format(i)\n",
    "    setattr(mod, model_,\n",
    "            BERTClassifier(bert_base, num_classes=2, dropout=0.3))\n",
    "\n",
    "    # 분류 레이어만 초기화 한다. \n",
    "    getattr(mod, model_).classifier.initialize(ctx=ctx)\n",
    "    getattr(mod, model_).hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 5e-5\n",
    "\n",
    "for i in range(1, 15):\n",
    "    setattr(mod, 'train_dataloader{}'.format(i),\n",
    "            mx.gluon.data.DataLoader(\n",
    "                getattr(mod, 'data_train{}'.format(i)), \n",
    "                batch_size=batch_size, num_workers=0))\n",
    "    setattr(mod, 'valid_dataloader{}'.format(i),\n",
    "            mx.gluon.data.DataLoader(\n",
    "                getattr(mod, 'data_valid{}'.format(i)), \n",
    "                batch_size=batch_size, num_workers=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 4\n",
    "num_epochs = 5\n",
    "\n",
    "for i in range(1, 15):\n",
    "    model_ = getattr(mod, 'model{}'.format(i))\n",
    "    setattr(mod, 'trainer{}'.format(i), gluon.Trainer(\n",
    "        model_.collect_params(), \n",
    "        'bertadam',\n",
    "        {'learning_rate': lr, \n",
    "         'epsilon': 1e-9, \n",
    "         'wd':0.01}))\n",
    "    # LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "    iters = model_.collect_params('.*beta|.*gamma|.*bias').items()\n",
    "    for _, v in iters:\n",
    "        v.wd_mult = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    model_ = getattr(mod, 'model{}'.format(i))\n",
    "    setattr(mod, 'params{}'.format(i),\n",
    "           [p for p in model_.collect_params().values() \n",
    "              if p.grad_req != 'null']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate warmup을 위한 준비 \n",
    "step_size = batch_size \n",
    "num_train_examples = len(data_train1)\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    i = 0\n",
    "    for i, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        i += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30804"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 모델\n",
      "[Epoch 1 Batch 50/482] loss=2.8177, lr=0.0000116667, acc=0.917\n",
      "[Epoch 1 Batch 100/482] loss=0.8137, lr=0.0000220833, acc=0.950\n",
      "[Epoch 1 Batch 150/482] loss=1.0392, lr=0.0000325000, acc=0.959\n",
      "[Epoch 1 Batch 200/482] loss=1.5004, lr=0.0000429167, acc=0.963\n",
      "[Epoch 1 Batch 250/482] loss=2.6393, lr=0.0000496307, acc=0.962\n",
      "[Epoch 1 Batch 300/482] loss=1.7526, lr=0.0000484765, acc=0.963\n",
      "[Epoch 1 Batch 350/482] loss=2.0649, lr=0.0000473223, acc=0.962\n",
      "[Epoch 1 Batch 400/482] loss=2.3171, lr=0.0000461681, acc=0.959\n",
      "[Epoch 1 Batch 450/482] loss=1.3803, lr=0.0000450139, acc=0.961\n",
      "\n",
      "Valid acc : 0.9848091404829914\n",
      "[Epoch 2 Batch 50/482] loss=1.0960, lr=0.0000431210, acc=0.982\n",
      "[Epoch 2 Batch 100/482] loss=1.4387, lr=0.0000419668, acc=0.974\n",
      "[Epoch 2 Batch 150/482] loss=0.8660, lr=0.0000408126, acc=0.978\n",
      "[Epoch 2 Batch 200/482] loss=0.4934, lr=0.0000396584, acc=0.981\n",
      "[Epoch 2 Batch 250/482] loss=1.2836, lr=0.0000385042, acc=0.978\n",
      "[Epoch 2 Batch 300/482] loss=1.5405, lr=0.0000373500, acc=0.976\n",
      "[Epoch 2 Batch 350/482] loss=0.7342, lr=0.0000361958, acc=0.977\n",
      "[Epoch 2 Batch 400/482] loss=0.7901, lr=0.0000350416, acc=0.978\n",
      "[Epoch 2 Batch 450/482] loss=0.5907, lr=0.0000338873, acc=0.980\n",
      "\n",
      "Valid acc : 0.990651778758764\n",
      "[Epoch 3 Batch 50/482] loss=0.6875, lr=0.0000319945, acc=0.989\n",
      "[Epoch 3 Batch 100/482] loss=0.7305, lr=0.0000308403, acc=0.988\n",
      "[Epoch 3 Batch 150/482] loss=0.7184, lr=0.0000296861, acc=0.989\n",
      "[Epoch 3 Batch 200/482] loss=0.6591, lr=0.0000285319, acc=0.989\n",
      "[Epoch 3 Batch 250/482] loss=0.5190, lr=0.0000273777, acc=0.990\n",
      "[Epoch 3 Batch 300/482] loss=0.5215, lr=0.0000262235, acc=0.990\n",
      "[Epoch 3 Batch 350/482] loss=0.3310, lr=0.0000250693, acc=0.990\n",
      "[Epoch 3 Batch 400/482] loss=0.7567, lr=0.0000239151, acc=0.990\n",
      "[Epoch 3 Batch 450/482] loss=0.5504, lr=0.0000227608, acc=0.990\n",
      "\n",
      "Valid acc : 0.9902622695403791\n",
      "[Epoch 4 Batch 50/482] loss=0.4662, lr=0.0000208680, acc=0.989\n",
      "[Epoch 4 Batch 100/482] loss=1.9933, lr=0.0000197138, acc=0.972\n",
      "[Epoch 4 Batch 150/482] loss=0.5637, lr=0.0000185596, acc=0.979\n",
      "[Epoch 4 Batch 200/482] loss=0.5407, lr=0.0000174054, acc=0.982\n",
      "[Epoch 4 Batch 250/482] loss=0.4607, lr=0.0000162512, acc=0.984\n",
      "[Epoch 4 Batch 300/482] loss=0.3885, lr=0.0000150970, acc=0.985\n",
      "[Epoch 4 Batch 350/482] loss=0.2421, lr=0.0000139428, acc=0.987\n",
      "[Epoch 4 Batch 400/482] loss=0.5118, lr=0.0000127886, acc=0.987\n",
      "[Epoch 4 Batch 450/482] loss=0.3347, lr=0.0000116343, acc=0.988\n",
      "\n",
      "Valid acc : 0.9919501428200468\n",
      "[Epoch 5 Batch 50/482] loss=0.4171, lr=0.0000097415, acc=0.993\n",
      "[Epoch 5 Batch 100/482] loss=0.2651, lr=0.0000085873, acc=0.994\n",
      "[Epoch 5 Batch 150/482] loss=0.2967, lr=0.0000074331, acc=0.994\n",
      "[Epoch 5 Batch 200/482] loss=0.4506, lr=0.0000062789, acc=0.994\n",
      "[Epoch 5 Batch 250/482] loss=0.2461, lr=0.0000051247, acc=0.994\n",
      "[Epoch 5 Batch 300/482] loss=0.3101, lr=0.0000039705, acc=0.994\n",
      "[Epoch 5 Batch 350/482] loss=0.2095, lr=0.0000028163, acc=0.995\n",
      "[Epoch 5 Batch 400/482] loss=0.5680, lr=0.0000016620, acc=0.994\n",
      "[Epoch 5 Batch 450/482] loss=0.3786, lr=0.0000005078, acc=0.994\n",
      "\n",
      "Valid acc : 0.994417034536484\n",
      "2번째 모델\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[14:01:22] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\storage\\./pooled_storage_manager.h:157: cudaMalloc failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-31227eb108ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallreduce_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_global_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\gluonnlp\\utils\\parameter.py\u001b[0m in \u001b[0;36mclip_grad_global_norm\u001b[1;34m(parameters, max_norm, check_isfinite)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_isfinite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             warnings.warn(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2014\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [14:01:22] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\storage\\./pooled_storage_manager.h:157: cudaMalloc failed: out of memory"
     ]
    }
   ],
   "source": [
    "for i in range(1, 15):\n",
    "    print('{}번째 모델'.format(i))\n",
    "    model = getattr(mod, 'model{}'.format(i))\n",
    "    trainer = getattr(mod, 'trainer{}'.format(i))\n",
    "    train_dataloader = getattr(mod, 'train_dataloader{}'.format(i))\n",
    "    valid_dataloader = getattr(mod, 'valid_dataloader{}'.format(i))\n",
    "    params = getattr(mod, 'params{}'.format(i))\n",
    "    for epoch_id in range(num_epochs):\n",
    "        metric.reset()\n",
    "        step_loss = 0\n",
    "        for batch_id, (token_ids, \n",
    "                       valid_length, \n",
    "                       segment_ids, \n",
    "                       label) in enumerate(train_dataloader):\n",
    "            step_num += 1\n",
    "            if step_num < num_warmup_steps:\n",
    "                new_lr = lr * step_num / num_warmup_steps\n",
    "            else:\n",
    "                offset = (step_num - num_warmup_steps) * lr / (\n",
    "                    num_train_steps - num_warmup_steps)\n",
    "                new_lr = lr - offset\n",
    "            trainer.set_learning_rate(new_lr)\n",
    "            with mx.autograd.record():\n",
    "                # load data to GPU\n",
    "                token_ids = token_ids.as_in_context(ctx)\n",
    "                valid_length = valid_length.as_in_context(ctx)\n",
    "                segment_ids = segment_ids.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "\n",
    "                # forward computation\n",
    "                out = model(token_ids, segment_ids, \n",
    "                            valid_length.astype('float32'))\n",
    "                ls = loss_function(out, label).mean()\n",
    "\n",
    "            # backward computation\n",
    "            ls.backward()\n",
    "            trainer.allreduce_grads()\n",
    "            nlp.utils.clip_grad_global_norm(params, 1)\n",
    "            trainer.update(token_ids.shape[0])\n",
    "\n",
    "            step_loss += ls.asscalar()\n",
    "            metric.update([label], [out])\n",
    "            if (batch_id + 1) % (50) == 0:\n",
    "                print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                             .format(epoch_id + 1, batch_id + 1, \n",
    "                                     len(train_dataloader),\n",
    "                                     step_loss / log_interval,\n",
    "                                     trainer.learning_rate, \n",
    "                                     metric.get()[1]))\n",
    "                step_loss = 0\n",
    "        valid_acc = evaluate_accuracy(model, valid_dataloader, ctx)\n",
    "        print('Valid acc : {}\\n'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('model1.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2번째 모델\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[14:18:56] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\operator\\nn\\../mxnet_op.h:805: Check failed: err == cudaSuccess (2 vs. 0) : Name: mxnet_generic_kernel_ex ErrStr:out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-5b2ecf9480f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallreduce_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_global_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\gluonnlp\\utils\\parameter.py\u001b[0m in \u001b[0;36mclip_grad_global_norm\u001b[1;34m(parameters, max_norm, check_isfinite)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_isfinite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             warnings.warn(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2014\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [14:18:56] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\operator\\nn\\../mxnet_op.h:805: Check failed: err == cudaSuccess (2 vs. 0) : Name: mxnet_generic_kernel_ex ErrStr:out of memory"
     ]
    }
   ],
   "source": [
    "for i in range(2, 15):\n",
    "    print('{}번째 모델'.format(i))\n",
    "    model = getattr(mod, 'model{}'.format(i))\n",
    "    trainer = getattr(mod, 'trainer{}'.format(i))\n",
    "    train_dataloader = getattr(mod, 'train_dataloader{}'.format(i))\n",
    "    valid_dataloader = getattr(mod, 'valid_dataloader{}'.format(i))\n",
    "    params = getattr(mod, 'params{}'.format(i))\n",
    "    for epoch_id in range(num_epochs):\n",
    "        metric.reset()\n",
    "        step_loss = 0\n",
    "        for batch_id, (token_ids, \n",
    "                       valid_length, \n",
    "                       segment_ids, \n",
    "                       label) in enumerate(train_dataloader):\n",
    "            step_num += 1\n",
    "            if step_num < num_warmup_steps:\n",
    "                new_lr = lr * step_num / num_warmup_steps\n",
    "            else:\n",
    "                offset = (step_num - num_warmup_steps) * lr / (\n",
    "                    num_train_steps - num_warmup_steps)\n",
    "                new_lr = lr - offset\n",
    "            trainer.set_learning_rate(new_lr)\n",
    "            with mx.autograd.record():\n",
    "                # load data to GPU\n",
    "                token_ids = token_ids.as_in_context(ctx)\n",
    "                valid_length = valid_length.as_in_context(ctx)\n",
    "                segment_ids = segment_ids.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "\n",
    "                # forward computation\n",
    "                out = model(token_ids, segment_ids, \n",
    "                            valid_length.astype('float32'))\n",
    "                ls = loss_function(out, label).mean()\n",
    "\n",
    "            # backward computation\n",
    "            ls.backward()\n",
    "            trainer.allreduce_grads()\n",
    "            nlp.utils.clip_grad_global_norm(params, 1)\n",
    "            trainer.update(token_ids.shape[0])\n",
    "\n",
    "            step_loss += ls.asscalar()\n",
    "            metric.update([label], [out])\n",
    "            if (batch_id + 1) % (50) == 0:\n",
    "                print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                             .format(epoch_id + 1, batch_id + 1, \n",
    "                                     len(train_dataloader),\n",
    "                                     step_loss / log_interval,\n",
    "                                     trainer.learning_rate, \n",
    "                                     metric.get()[1]))\n",
    "                step_loss = 0\n",
    "        valid_acc = evaluate_accuracy(model, valid_dataloader, ctx)\n",
    "        print('Valid acc : {}\\n'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = BERTClassifier(bert_base, num_classes=2, dropout=0.3)\n",
    "new_net.load_parameters('model1.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BERTClassifier' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-e6d76aa198a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BERTClassifier' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "model1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
